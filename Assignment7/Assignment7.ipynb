{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment7.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XLph9STCSrl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#importing requried libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import LSTM\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import np_utils\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils.data_utils import get_file\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9SozSR_CfdE",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "Ly8gQ29weXJpZ2h0IDIwMTcgR29vZ2xlIExMQwovLwovLyBMaWNlbnNlZCB1bmRlciB0aGUgQXBhY2hlIExpY2Vuc2UsIFZlcnNpb24gMi4wICh0aGUgIkxpY2Vuc2UiKTsKLy8geW91IG1heSBub3QgdXNlIHRoaXMgZmlsZSBleGNlcHQgaW4gY29tcGxpYW5jZSB3aXRoIHRoZSBMaWNlbnNlLgovLyBZb3UgbWF5IG9idGFpbiBhIGNvcHkgb2YgdGhlIExpY2Vuc2UgYXQKLy8KLy8gICAgICBodHRwOi8vd3d3LmFwYWNoZS5vcmcvbGljZW5zZXMvTElDRU5TRS0yLjAKLy8KLy8gVW5sZXNzIHJlcXVpcmVkIGJ5IGFwcGxpY2FibGUgbGF3IG9yIGFncmVlZCB0byBpbiB3cml0aW5nLCBzb2Z0d2FyZQovLyBkaXN0cmlidXRlZCB1bmRlciB0aGUgTGljZW5zZSBpcyBkaXN0cmlidXRlZCBvbiBhbiAiQVMgSVMiIEJBU0lTLAovLyBXSVRIT1VUIFdBUlJBTlRJRVMgT1IgQ09ORElUSU9OUyBPRiBBTlkgS0lORCwgZWl0aGVyIGV4cHJlc3Mgb3IgaW1wbGllZC4KLy8gU2VlIHRoZSBMaWNlbnNlIGZvciB0aGUgc3BlY2lmaWMgbGFuZ3VhZ2UgZ292ZXJuaW5nIHBlcm1pc3Npb25zIGFuZAovLyBsaW1pdGF0aW9ucyB1bmRlciB0aGUgTGljZW5zZS4KCi8qKgogKiBAZmlsZW92ZXJ2aWV3IEhlbHBlcnMgZm9yIGdvb2dsZS5jb2xhYiBQeXRob24gbW9kdWxlLgogKi8KKGZ1bmN0aW9uKHNjb3BlKSB7CmZ1bmN0aW9uIHNwYW4odGV4dCwgc3R5bGVBdHRyaWJ1dGVzID0ge30pIHsKICBjb25zdCBlbGVtZW50ID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnc3BhbicpOwogIGVsZW1lbnQudGV4dENvbnRlbnQgPSB0ZXh0OwogIGZvciAoY29uc3Qga2V5IG9mIE9iamVjdC5rZXlzKHN0eWxlQXR0cmlidXRlcykpIHsKICAgIGVsZW1lbnQuc3R5bGVba2V5XSA9IHN0eWxlQXR0cmlidXRlc1trZXldOwogIH0KICByZXR1cm4gZWxlbWVudDsKfQoKLy8gTWF4IG51bWJlciBvZiBieXRlcyB3aGljaCB3aWxsIGJlIHVwbG9hZGVkIGF0IGEgdGltZS4KY29uc3QgTUFYX1BBWUxPQURfU0laRSA9IDEwMCAqIDEwMjQ7Ci8vIE1heCBhbW91bnQgb2YgdGltZSB0byBibG9jayB3YWl0aW5nIGZvciB0aGUgdXNlci4KY29uc3QgRklMRV9DSEFOR0VfVElNRU9VVF9NUyA9IDMwICogMTAwMDsKCmZ1bmN0aW9uIF91cGxvYWRGaWxlcyhpbnB1dElkLCBvdXRwdXRJZCkgewogIGNvbnN0IHN0ZXBzID0gdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKTsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIC8vIENhY2hlIHN0ZXBzIG9uIHRoZSBvdXRwdXRFbGVtZW50IHRvIG1ha2UgaXQgYXZhaWxhYmxlIGZvciB0aGUgbmV4dCBjYWxsCiAgLy8gdG8gdXBsb2FkRmlsZXNDb250aW51ZSBmcm9tIFB5dGhvbi4KICBvdXRwdXRFbGVtZW50LnN0ZXBzID0gc3RlcHM7CgogIHJldHVybiBfdXBsb2FkRmlsZXNDb250aW51ZShvdXRwdXRJZCk7Cn0KCi8vIFRoaXMgaXMgcm91Z2hseSBhbiBhc3luYyBnZW5lcmF0b3IgKG5vdCBzdXBwb3J0ZWQgaW4gdGhlIGJyb3dzZXIgeWV0KSwKLy8gd2hlcmUgdGhlcmUgYXJlIG11bHRpcGxlIGFzeW5jaHJvbm91cyBzdGVwcyBhbmQgdGhlIFB5dGhvbiBzaWRlIGlzIGdvaW5nCi8vIHRvIHBvbGwgZm9yIGNvbXBsZXRpb24gb2YgZWFjaCBzdGVwLgovLyBUaGlzIHVzZXMgYSBQcm9taXNlIHRvIGJsb2NrIHRoZSBweXRob24gc2lkZSBvbiBjb21wbGV0aW9uIG9mIGVhY2ggc3RlcCwKLy8gdGhlbiBwYXNzZXMgdGhlIHJlc3VsdCBvZiB0aGUgcHJldmlvdXMgc3RlcCBhcyB0aGUgaW5wdXQgdG8gdGhlIG5leHQgc3RlcC4KZnVuY3Rpb24gX3VwbG9hZEZpbGVzQ29udGludWUob3V0cHV0SWQpIHsKICBjb25zdCBvdXRwdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQob3V0cHV0SWQpOwogIGNvbnN0IHN0ZXBzID0gb3V0cHV0RWxlbWVudC5zdGVwczsKCiAgY29uc3QgbmV4dCA9IHN0ZXBzLm5leHQob3V0cHV0RWxlbWVudC5sYXN0UHJvbWlzZVZhbHVlKTsKICByZXR1cm4gUHJvbWlzZS5yZXNvbHZlKG5leHQudmFsdWUucHJvbWlzZSkudGhlbigodmFsdWUpID0+IHsKICAgIC8vIENhY2hlIHRoZSBsYXN0IHByb21pc2UgdmFsdWUgdG8gbWFrZSBpdCBhdmFpbGFibGUgdG8gdGhlIG5leHQKICAgIC8vIHN0ZXAgb2YgdGhlIGdlbmVyYXRvci4KICAgIG91dHB1dEVsZW1lbnQubGFzdFByb21pc2VWYWx1ZSA9IHZhbHVlOwogICAgcmV0dXJuIG5leHQudmFsdWUucmVzcG9uc2U7CiAgfSk7Cn0KCi8qKgogKiBHZW5lcmF0b3IgZnVuY3Rpb24gd2hpY2ggaXMgY2FsbGVkIGJldHdlZW4gZWFjaCBhc3luYyBzdGVwIG9mIHRoZSB1cGxvYWQKICogcHJvY2Vzcy4KICogQHBhcmFtIHtzdHJpbmd9IGlucHV0SWQgRWxlbWVudCBJRCBvZiB0aGUgaW5wdXQgZmlsZSBwaWNrZXIgZWxlbWVudC4KICogQHBhcmFtIHtzdHJpbmd9IG91dHB1dElkIEVsZW1lbnQgSUQgb2YgdGhlIG91dHB1dCBkaXNwbGF5LgogKiBAcmV0dXJuIHshSXRlcmFibGU8IU9iamVjdD59IEl0ZXJhYmxlIG9mIG5leHQgc3RlcHMuCiAqLwpmdW5jdGlvbiogdXBsb2FkRmlsZXNTdGVwKGlucHV0SWQsIG91dHB1dElkKSB7CiAgY29uc3QgaW5wdXRFbGVtZW50ID0gZG9jdW1lbnQuZ2V0RWxlbWVudEJ5SWQoaW5wdXRJZCk7CiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gZmFsc2U7CgogIGNvbnN0IG91dHB1dEVsZW1lbnQgPSBkb2N1bWVudC5nZXRFbGVtZW50QnlJZChvdXRwdXRJZCk7CiAgb3V0cHV0RWxlbWVudC5pbm5lckhUTUwgPSAnJzsKCiAgY29uc3QgcGlja2VkUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBpbnB1dEVsZW1lbnQuYWRkRXZlbnRMaXN0ZW5lcignY2hhbmdlJywgKGUpID0+IHsKICAgICAgcmVzb2x2ZShlLnRhcmdldC5maWxlcyk7CiAgICB9KTsKICB9KTsKCiAgY29uc3QgY2FuY2VsID0gZG9jdW1lbnQuY3JlYXRlRWxlbWVudCgnYnV0dG9uJyk7CiAgaW5wdXRFbGVtZW50LnBhcmVudEVsZW1lbnQuYXBwZW5kQ2hpbGQoY2FuY2VsKTsKICBjYW5jZWwudGV4dENvbnRlbnQgPSAnQ2FuY2VsIHVwbG9hZCc7CiAgY29uc3QgY2FuY2VsUHJvbWlzZSA9IG5ldyBQcm9taXNlKChyZXNvbHZlKSA9PiB7CiAgICBjYW5jZWwub25jbGljayA9ICgpID0+IHsKICAgICAgcmVzb2x2ZShudWxsKTsKICAgIH07CiAgfSk7CgogIC8vIENhbmNlbCB1cGxvYWQgaWYgdXNlciBoYXNuJ3QgcGlja2VkIGFueXRoaW5nIGluIHRpbWVvdXQuCiAgY29uc3QgdGltZW91dFByb21pc2UgPSBuZXcgUHJvbWlzZSgocmVzb2x2ZSkgPT4gewogICAgc2V0VGltZW91dCgoKSA9PiB7CiAgICAgIHJlc29sdmUobnVsbCk7CiAgICB9LCBGSUxFX0NIQU5HRV9USU1FT1VUX01TKTsKICB9KTsKCiAgLy8gV2FpdCBmb3IgdGhlIHVzZXIgdG8gcGljayB0aGUgZmlsZXMuCiAgY29uc3QgZmlsZXMgPSB5aWVsZCB7CiAgICBwcm9taXNlOiBQcm9taXNlLnJhY2UoW3BpY2tlZFByb21pc2UsIHRpbWVvdXRQcm9taXNlLCBjYW5jZWxQcm9taXNlXSksCiAgICByZXNwb25zZTogewogICAgICBhY3Rpb246ICdzdGFydGluZycsCiAgICB9CiAgfTsKCiAgaWYgKCFmaWxlcykgewogICAgcmV0dXJuIHsKICAgICAgcmVzcG9uc2U6IHsKICAgICAgICBhY3Rpb246ICdjb21wbGV0ZScsCiAgICAgIH0KICAgIH07CiAgfQoKICBjYW5jZWwucmVtb3ZlKCk7CgogIC8vIERpc2FibGUgdGhlIGlucHV0IGVsZW1lbnQgc2luY2UgZnVydGhlciBwaWNrcyBhcmUgbm90IGFsbG93ZWQuCiAgaW5wdXRFbGVtZW50LmRpc2FibGVkID0gdHJ1ZTsKCiAgZm9yIChjb25zdCBmaWxlIG9mIGZpbGVzKSB7CiAgICBjb25zdCBsaSA9IGRvY3VtZW50LmNyZWF0ZUVsZW1lbnQoJ2xpJyk7CiAgICBsaS5hcHBlbmQoc3BhbihmaWxlLm5hbWUsIHtmb250V2VpZ2h0OiAnYm9sZCd9KSk7CiAgICBsaS5hcHBlbmQoc3BhbigKICAgICAgICBgKCR7ZmlsZS50eXBlIHx8ICduL2EnfSkgLSAke2ZpbGUuc2l6ZX0gYnl0ZXMsIGAgKwogICAgICAgIGBsYXN0IG1vZGlmaWVkOiAkewogICAgICAgICAgICBmaWxlLmxhc3RNb2RpZmllZERhdGUgPyBmaWxlLmxhc3RNb2RpZmllZERhdGUudG9Mb2NhbGVEYXRlU3RyaW5nKCkgOgogICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAgICAnbi9hJ30gLSBgKSk7CiAgICBjb25zdCBwZXJjZW50ID0gc3BhbignMCUgZG9uZScpOwogICAgbGkuYXBwZW5kQ2hpbGQocGVyY2VudCk7CgogICAgb3V0cHV0RWxlbWVudC5hcHBlbmRDaGlsZChsaSk7CgogICAgY29uc3QgZmlsZURhdGFQcm9taXNlID0gbmV3IFByb21pc2UoKHJlc29sdmUpID0+IHsKICAgICAgY29uc3QgcmVhZGVyID0gbmV3IEZpbGVSZWFkZXIoKTsKICAgICAgcmVhZGVyLm9ubG9hZCA9IChlKSA9PiB7CiAgICAgICAgcmVzb2x2ZShlLnRhcmdldC5yZXN1bHQpOwogICAgICB9OwogICAgICByZWFkZXIucmVhZEFzQXJyYXlCdWZmZXIoZmlsZSk7CiAgICB9KTsKICAgIC8vIFdhaXQgZm9yIHRoZSBkYXRhIHRvIGJlIHJlYWR5LgogICAgbGV0IGZpbGVEYXRhID0geWllbGQgewogICAgICBwcm9taXNlOiBmaWxlRGF0YVByb21pc2UsCiAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgYWN0aW9uOiAnY29udGludWUnLAogICAgICB9CiAgICB9OwoKICAgIC8vIFVzZSBhIGNodW5rZWQgc2VuZGluZyB0byBhdm9pZCBtZXNzYWdlIHNpemUgbGltaXRzLiBTZWUgYi82MjExNTY2MC4KICAgIGxldCBwb3NpdGlvbiA9IDA7CiAgICB3aGlsZSAocG9zaXRpb24gPCBmaWxlRGF0YS5ieXRlTGVuZ3RoKSB7CiAgICAgIGNvbnN0IGxlbmd0aCA9IE1hdGgubWluKGZpbGVEYXRhLmJ5dGVMZW5ndGggLSBwb3NpdGlvbiwgTUFYX1BBWUxPQURfU0laRSk7CiAgICAgIGNvbnN0IGNodW5rID0gbmV3IFVpbnQ4QXJyYXkoZmlsZURhdGEsIHBvc2l0aW9uLCBsZW5ndGgpOwogICAgICBwb3NpdGlvbiArPSBsZW5ndGg7CgogICAgICBjb25zdCBiYXNlNjQgPSBidG9hKFN0cmluZy5mcm9tQ2hhckNvZGUuYXBwbHkobnVsbCwgY2h1bmspKTsKICAgICAgeWllbGQgewogICAgICAgIHJlc3BvbnNlOiB7CiAgICAgICAgICBhY3Rpb246ICdhcHBlbmQnLAogICAgICAgICAgZmlsZTogZmlsZS5uYW1lLAogICAgICAgICAgZGF0YTogYmFzZTY0LAogICAgICAgIH0sCiAgICAgIH07CiAgICAgIHBlcmNlbnQudGV4dENvbnRlbnQgPQogICAgICAgICAgYCR7TWF0aC5yb3VuZCgocG9zaXRpb24gLyBmaWxlRGF0YS5ieXRlTGVuZ3RoKSAqIDEwMCl9JSBkb25lYDsKICAgIH0KICB9CgogIC8vIEFsbCBkb25lLgogIHlpZWxkIHsKICAgIHJlc3BvbnNlOiB7CiAgICAgIGFjdGlvbjogJ2NvbXBsZXRlJywKICAgIH0KICB9Owp9CgpzY29wZS5nb29nbGUgPSBzY29wZS5nb29nbGUgfHwge307CnNjb3BlLmdvb2dsZS5jb2xhYiA9IHNjb3BlLmdvb2dsZS5jb2xhYiB8fCB7fTsKc2NvcGUuZ29vZ2xlLmNvbGFiLl9maWxlcyA9IHsKICBfdXBsb2FkRmlsZXMsCiAgX3VwbG9hZEZpbGVzQ29udGludWUsCn07Cn0pKHNlbGYpOwo=",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 83
        },
        "outputId": "a6958276-1d3a-47b1-8aa4-b93542e8b03c"
      },
      "source": [
        "# Uploading data.txt file which contains works of Shakespeare\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-cf4c37ef-905e-43f3-babd-1613059f3f38\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-cf4c37ef-905e-43f3-babd-1613059f3f38\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data.txt to data.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGKFemUbDwOK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Reading the file and converting all the capital words to lower\n",
        "text = open('data.txt').read().lower()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aAk10svUMqfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "removal_list = [\"\\xbb\", \"\\xbf\", \"\\xef\"]\n",
        "for char_to_remove in removal_list:\n",
        "  text = text.replace(char_to_remove, \" \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nwQURKv8Cfe8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9f906cc-c67e-44a0-db04-32d1807c1be9"
      },
      "source": [
        "text = text[:int(len(text)/20)]\n",
        "print('corpus length:', len(text))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "corpus length: 228666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jtk7e-IIGDHF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chars = sorted(list(set(text)))                               # Taking the set of the characters in the text\n",
        "char_indices = dict((c, i) for i, c in enumerate(chars))      # Defining the dictionaries to map characters to indices and vice versa \n",
        "indices_char = dict((i, c) for i, c in enumerate(chars))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Owb8YOa5GU81",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a427065c-b013-4777-c4d4-2f7c83a6688c"
      },
      "source": [
        "# From dataset take the pair of sentence (length = maxlen) and next character and convert it into index form\n",
        "\n",
        "maxlen = 200\n",
        "step = 5\n",
        "sentences = []\n",
        "next_chars = []\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    Input = text[i: i + maxlen]\n",
        "    NextChar = text[i+maxlen]\n",
        "    sentences.append( [char_indices[char] for char in Input])\n",
        "    next_chars.append( [char_indices[char] for char in NextChar])\n",
        "\n",
        "    \n",
        "print('nb sequences:', len(sentences))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nb sequences: 45694\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X9eH95RLQM_W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# One hot encoding\n",
        "X = np.reshape(sentences, (len(sentences), maxlen, 1))\n",
        "y = np_utils.to_categorical(next_chars)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94NoVaTVIACW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# RNN Model\n",
        "model = Sequential()\n",
        "model.add(LSTM(512, return_sequences=True, input_shape=(X.shape[1],X.shape[2])))\n",
        "model.add(LSTM((512), input_shape=(X.shape[1],X.shape[2])))\n",
        "model.add(Dense(len(chars)))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=Adam())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBldYwD7IAEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 729
        },
        "outputId": "405a43d0-64b0-421b-a6d6-9b6a9f9e4415"
      },
      "source": [
        "model.fit(X, y, epochs=20, batch_size=128)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "45694/45694 [==============================] - 187s 4ms/step - loss: 2.8169\n",
            "Epoch 2/20\n",
            "45694/45694 [==============================] - 184s 4ms/step - loss: 2.5430\n",
            "Epoch 3/20\n",
            "45694/45694 [==============================] - 185s 4ms/step - loss: 2.3446\n",
            "Epoch 4/20\n",
            "45694/45694 [==============================] - 185s 4ms/step - loss: 2.1961\n",
            "Epoch 5/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 2.0715\n",
            "Epoch 6/20\n",
            "45694/45694 [==============================] - 185s 4ms/step - loss: 1.9608\n",
            "Epoch 7/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 1.8671\n",
            "Epoch 8/20\n",
            "45694/45694 [==============================] - 187s 4ms/step - loss: 1.7726\n",
            "Epoch 9/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 1.6766\n",
            "Epoch 10/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 1.5773\n",
            "Epoch 11/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 1.4738\n",
            "Epoch 12/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 1.3588\n",
            "Epoch 13/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 1.2360\n",
            "Epoch 14/20\n",
            "45694/45694 [==============================] - 185s 4ms/step - loss: 1.1054\n",
            "Epoch 15/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 0.9699\n",
            "Epoch 16/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 0.8210\n",
            "Epoch 17/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 0.6786\n",
            "Epoch 18/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 0.5379\n",
            "Epoch 19/20\n",
            "45694/45694 [==============================] - 185s 4ms/step - loss: 0.4142\n",
            "Epoch 20/20\n",
            "45694/45694 [==============================] - 186s 4ms/step - loss: 0.2992\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f12beebcb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aEPCojwpM-c_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "453351de-f8cd-4028-b9fa-48a86468b3bf"
      },
      "source": [
        "# Generating the characters from a random start sentence from dataset\n",
        "# maxlen = 40, epochs = 5, steps = 100, LSTM Layers = 128\n",
        "\n",
        "randomStart = sentences[7]\n",
        "M = randomStart\n",
        "print(\"\".join([indices_char[value] for value in randomStart]))\n",
        "  \n",
        "\n",
        "for i in range(100):\n",
        "  x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
        "  pred = model.predict(x)\n",
        "  index = np.argmax(pred)\n",
        "  \n",
        "  M.append(index)\n",
        "  randomStart = M[i+1:len(M)]  \n",
        "  #print(\"\")\n",
        "  print(indices_char[index], end=\"\")"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nk we are too dear: the leanness that\n",
            "af\n",
            "d the the the the the the the the the the the the the the the the the the the the the the the the th"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYR0l04r0h9I",
        "colab_type": "text"
      },
      "source": [
        "**Model 1**\n",
        "\n",
        "Maximum length of input = 40 \n",
        "\n",
        "Number of epoch = 5\n",
        "\n",
        "Steps = 100\n",
        "\n",
        "LSTM Layers = 128\n",
        "\n",
        "It gives no significant output and just generates \"the\" words sequentially.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-WzS2DmIAHl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "36829751-10ab-439a-cd07-a814f1118c53"
      },
      "source": [
        "# Generating the characters from a random start sentence from dataset\n",
        "# maxlen = 100, epochs = 10, steps = 100, LSTM Layers = 128\n",
        "\n",
        "randomStart = sentences[7]\n",
        "M = randomStart\n",
        "print(\"\".join([indices_char[value] for value in randomStart]))\n",
        "  \n",
        "\n",
        "for i in range(100):\n",
        "  x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
        "  pred = model.predict(x)\n",
        "  index = np.argmax(pred)\n",
        "  \n",
        "  M.append(index)\n",
        "  randomStart = M[i+1:len(M)]  \n",
        "  #print(\"\")\n",
        "  print(indices_char[index], end=\"\")"
      ],
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "nk we are too dear: the leanness that\n",
            "afflicts us, the object of our misery, is as an\n",
            "inventory to p\n",
            "$$$[$$$$$$$[$$3$$$$$[$$$$$$$[$$$[$$$$$$$$3$$$$$$$3$$3$$$[[$[$$$[$$$$$[$$$$$3$$$$3$$3$$3$$3$[$[$$$$[$"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HM2LhnBIDx1V",
        "colab_type": "text"
      },
      "source": [
        "**Model 2**\n",
        "\n",
        "Maximum length of input = 100\n",
        "\n",
        "Number of epoch = 10\n",
        "\n",
        "Steps = 100\n",
        "\n",
        "LSTM Layers = 128\n",
        "\n",
        "The output is worse than model 1, just generates $ signs.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLHAaJPOKWRm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "ec8066c9-e39e-4910-b522-4ade16b25a27"
      },
      "source": [
        "# Generating the characters from a random start sentence from dataset\n",
        "# maxlen = 100, epochs = 4, steps = 20, LSTM Layers = 256\n",
        "\n",
        "randomStart = sentences[7]\n",
        "M = randomStart\n",
        "print(\"\".join([indices_char[value] for value in randomStart]))\n",
        "  \n",
        "\n",
        "for i in range(100):\n",
        "  x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
        "  pred = model.predict(x)\n",
        "  index = np.argmax(pred)\n",
        "  \n",
        "  M.append(index)\n",
        "  randomStart = M[i+1:len(M)]  \n",
        "  #print(\"\")\n",
        "  print(indices_char[index], end=\"\")"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "famish?\n",
            "\n",
            "all:\n",
            "resolved. resolved.\n",
            "\n",
            "first citizen:\n",
            "first, you know caius marcius is chief enemy to th\n",
            "e she she shee and the siu ar the shee and the siu ar the shee and the siu ar the shee and the siu a"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdO0t5w-KW3O",
        "colab_type": "text"
      },
      "source": [
        "**Model 3**\n",
        "\n",
        "Maximum length of input = 100\n",
        "\n",
        "Number of epoch = 4\n",
        "\n",
        "Steps = 20\n",
        "\n",
        "LSTM Layers = 256\n",
        "\n",
        "The output is better compared to earlier models, but still it doen't generate something meaningful\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyXocm1OMiYX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "5e2b589a-64da-437c-c8c7-7c253e6ced15"
      },
      "source": [
        "# Generating the characters from a random start sentence from dataset\n",
        "# maxlen = 100, epochs = 10,  LSTM Layers = 512 \n",
        "\n",
        "randomStart = sentences[100]\n",
        "M = randomStart\n",
        "print(\"\".join([indices_char[value] for value in randomStart]))\n",
        "  \n",
        "\n",
        "for i in range(100):\n",
        "  x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
        "  pred = model.predict(x)\n",
        "  index = np.argmax(pred)\n",
        "  \n",
        "  M.append(index)\n",
        "  randomStart = M[i+1:len(M)]  \n",
        "  #print(\"\")\n",
        "  print(indices_char[index], end=\"\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " kill him, and we'll have corn at our own price.\n",
            "is't a verdict?\n",
            "\n",
            "all:\n",
            "no more talking on't; let it \n",
            "the stees hooours:\n",
            "\n",
            "second citizen:\n",
            "what shou should be ione work him of him.\n",
            "\n",
            "menenius:\n",
            "i am a kott"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BfxUEQpHcscH",
        "colab_type": "text"
      },
      "source": [
        "**Model 4**\n",
        "\n",
        "Maximum length of input = 100\n",
        "\n",
        "Number of epoch = 10\n",
        "\n",
        "LSTM Layers = 512\n",
        "\n",
        "Instead of one LSTM, two layers are used.\n",
        "\n",
        "The output is somewhat more meaningful compared to all earlier models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSlP3bGMdGHR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "9e8d0d59-7f16-4d55-cac7-ded3a2fc89b6"
      },
      "source": [
        "# Generating the characters from a random start sentence from dataset\n",
        "# maxlen = 200, epochs = 10,  LSTM Layers = 512\n",
        "\n",
        "randomStart = sentences[100]\n",
        "M = randomStart\n",
        "print(\"\".join([indices_char[value] for value in randomStart]))\n",
        "  \n",
        "\n",
        "for i in range(100):\n",
        "  x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
        "  pred = model.predict(x)\n",
        "  index = np.argmax(pred)\n",
        "  \n",
        "  M.append(index)\n",
        "  randomStart = M[i+1:len(M)]  \n",
        "  #print(\"\")\n",
        "  print(indices_char[index], end=\"\")"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " citizens, the patricians good.\n",
            "what authority surfeits on would relieve us: if they\n",
            "would yield us but the superfluity, while it were\n",
            "wholesome, we might guess they relieved us humanely;\n",
            "but they thi\n",
            "ngs you thin mekt rhall be meet not and hnure\n",
            "the baste of knntinning, we did nor marcius,\n",
            "they co s"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rX-4WELMQ3dL",
        "colab_type": "text"
      },
      "source": [
        "**Model 5**\n",
        "\n",
        "Maximum length of input = 200\n",
        "\n",
        "Number of epoch = 10\n",
        "\n",
        "LSTM Layers = 512\n",
        "\n",
        "Instead of one LSTM, two layers are used.\n",
        "\n",
        "The output is best among all earlier models\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7CAM8g4G-JZe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "f46ea9f0-4e92-48d6-d612-109ff42a4e32"
      },
      "source": [
        "# Generating the characters from a random start sentence from dataset\n",
        "# maxlen = 200, epochs = 20,  LSTM Layers = 512\n",
        "\n",
        "randomStart = sentences[315]\n",
        "M = randomStart\n",
        "print(\"\".join([indices_char[value] for value in randomStart]))\n",
        "  \n",
        "\n",
        "for i in range(100):\n",
        "  x = np.reshape(randomStart, (1, len(randomStart), 1))\n",
        "  pred = model.predict(x)\n",
        "  index = np.argmax(pred)\n",
        "  \n",
        "  M.append(index)\n",
        "  randomStart = M[i+1:len(M)]  \n",
        "  #print(\"\")\n",
        "  print(indices_char[index], end=\"\")"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "proud; which he\n",
            "is, even till the altitude of his virtue.\n",
            "\n",
            "second citizen:\n",
            "what he cannot help in his nature, you account a\n",
            "vice in him. you must in no way say he is covetous.\n",
            "\n",
            "first citizen:\n",
            "if i mus\n",
            "t not, as the blostes me the marcius.\n",
            "\n",
            "second citizen:\n",
            "nay, you seroeet have i cm rrott of you to th"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73pGklnG_Se9",
        "colab_type": "text"
      },
      "source": [
        "**Model 6**\n",
        "\n",
        "Maximum length of input = 200\n",
        "\n",
        "Number of epoch = 20\n",
        "\n",
        "LSTM Layers = 512\n",
        "\n",
        "Instead of one LSTM, two layers are used.\n",
        "\n",
        "The final optimized model"
      ]
    }
  ]
}